<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <title>æ‰‹èªè¾¨è­˜ç³»çµ± (ç¶²é ç‰ˆ)</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
    
    <style>
        body { text-align: center; background-color: #333; color: white; font-family: sans-serif; }
        #container { position: relative; display: inline-block; }
        video { display: none; } /* éš±è—åŸå§‹å½±ç‰‡ï¼Œåªé¡¯ç¤ºç•«å¥½éª¨æ¶çš„ Canvas */
        canvas { border: 5px solid #fff; border-radius: 10px; max-width: 100%; }
        #result { font-size: 2rem; margin-top: 20px; color: #00ff00; font-weight: bold; }
    </style>
</head>
<body>

    <h1>ğŸ¤Ÿ å³æ™‚æ‰‹èªè¾¨è­˜</h1>
    <div id="container">
        <video id="input_video"></video>
        <canvas id="output_canvas" width="640" height="480"></canvas>
    </div>
    <div id="result">æ¨¡å‹è¼‰å…¥ä¸­...</div>

    <script>
        // -----------------------------------------------------------
        // 1. è«‹æŠŠå‰›å‰›ç”¨ Python æŸ¥åˆ°çš„æ¨™ç±¤é †åºè²¼åœ¨é€™è£¡ï¼
        // -----------------------------------------------------------
        const LABELS = [
            "ä½ å¥½", "è¬è¬", "å¤§å®¶", "åˆå®‰" // <--- é€™è£¡ä¸€å®šè¦æ”¹æˆæ‚¨çš„çœŸæ­£æ¨™ç±¤
        ];

        const videoElement = document.getElementById('input_video');
        const canvasElement = document.getElementById('output_canvas');
        const canvasCtx = canvasElement.getContext('2d');
        const resultText = document.getElementById('result');
        
        let model;
        let sequence = []; // ç”¨ä¾†å­˜é€£çºŒå‹•ä½œ (è·Ÿ Python çš„ sequence ä¸€æ¨£)

        // 2. è¼‰å…¥æ¨¡å‹
        async function loadModel() {
            try {
                // è®€å–è½‰æ›å¥½çš„ model.json
                model = await tf.loadLayersModel('./web_model/model.json');
                resultText.innerText = "æº–å‚™å°±ç·’ï¼Œè«‹æ¯”æ‰‹èªï¼";
                console.log("æ¨¡å‹è¼‰å…¥æˆåŠŸ");
            } catch (error) {
                resultText.innerText = "æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘";
                console.error(error);
            }
        }

        // 3. è™•ç†æ¯ä¸€å¹€ç•«é¢ (MediaPipe çµæœ)
        function onResults(results) {
            // ç¹ªè£½èƒŒæ™¯èˆ‡éª¨æ¶
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
            
            if (results.multiHandLandmarks) {
                for (const landmarks of results.multiHandLandmarks) {
                    drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {color: '#00FF00', lineWidth: 5});
                    drawLandmarks(canvasCtx, landmarks, {color: '#FF0000', lineWidth: 2});
                }
                
                // --- é€™è£¡é–‹å§‹æ˜¯è¾¨è­˜é‚è¼¯ (æ¨¡æ“¬ Python çš„ extract_feature) ---
                if(model) {
                    let features = [];
                    // é€™è£¡ç°¡åŒ–è™•ç†ï¼šåªå–ç¬¬ä¸€éš»æ‰‹ï¼Œå°‡ 21 é» (x,y,z) æ”¤å¹³
                    // æ³¨æ„ï¼šé€™è£¡çš„é‚è¼¯å¿…é ˆè·Ÿæ‚¨ Python è¨“ç·´æ™‚ä¸€æ¨¡ä¸€æ¨£ï¼Œä¸ç„¶æœƒä¸æº–
                    // å¦‚æœæ‚¨ Python æœ‰æŠ“å…©éš»æ‰‹ï¼Œé€™è£¡ä¹Ÿè¦æ”¹å¯«
                    for(let i=0; i<results.multiHandLandmarks.length; i++){
                         // ç°¡å–®ç¤ºç¯„ï¼šæŠŠæ‰€æœ‰é»çš„ x, y, z åŠ å…¥é™£åˆ—
                         for(let lm of results.multiHandLandmarks[i]){
                             features.push(lm.x, lm.y, lm.z);
                         }
                    }
                    
                    // è£œé›¶ (å¦‚æœæ‚¨çš„æ¨¡å‹è¼¸å…¥é•·åº¦æ˜¯å›ºå®šçš„ï¼Œæ¯”å¦‚ 126ï¼Œä¸è¶³è¦è£œ0)
                    while (features.length < 126) features.push(0); 

                    // å­˜å…¥ä½‡åˆ— (Sequence)
                    sequence.push(features);
                    if(sequence.length > 30) sequence.shift(); // ä¿æŒ 30 å¹€

                    // é€²è¡Œé æ¸¬
                    if(sequence.length === 30) {
                        const inputTensor = tf.tensor([sequence]); // è½‰æˆ Tensor
                        const prediction = model.predict(inputTensor);
                        const data = prediction.dataSync(); // å–å¾—æ•¸å€¼
                        const maxIndex = data.indexOf(Math.max(...data)); // æ‰¾æœ€å¤§å€¼ç´¢å¼•
                        
                        // é¡¯ç¤ºçµæœ
                        if(data[maxIndex] > 0.7) { // ä¿¡å¿ƒåº¦ > 0.7
                            resultText.innerText = "è¾¨è­˜çµæœ: " + LABELS[maxIndex];
                        }
                    }
                }
            }
            canvasCtx.restore();
        }

        // 4. å•Ÿå‹•æ”å½±æ©Ÿèˆ‡ MediaPipe
        const hands = new Hands({locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
        }});
        hands.setOptions({
            maxNumHands: 2,
            modelComplexity: 1,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        hands.onResults(onResults);

        const camera = new Camera(videoElement, {
            onFrame: async () => {
                await hands.send({image: videoElement});
            },
            width: 640,
            height: 480
        });
        
        loadModel(); // å…ˆè¼‰å…¥æ¨¡å‹
        camera.start(); // å•Ÿå‹•æ”å½±æ©Ÿ
    </script>
</body>

</html>

